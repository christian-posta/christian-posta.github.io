---
layout: post
title: "Cloud Native Applications and the CAP Theorem"
modified:
categories: 
comments: true
tags: []
image:
  feature:
date: 2017-01-28T21:15:31-07:00
---

Building applications for enterprises has come a long way. To simplify a bit, we've gone from simply automating paper processes to using it as an alternative channel for our businesses to becoming the business itself.  Parallel to this we've seen the evolution of Moore's law from cheaper, faster individual hardware to fast, cheap, piles of hardware on demand. If our applications "are the business" and we have access to fleets of cheap machines and infrastructure, what does this mean for the applications and developers in this new era? It means we have to innovate by making changes quickly. We have to build resilient systems. We need to scale to deal with challenges like the world of IoT, big data, etc. Building systems and applications that fit this "cloud era" are a bit different than how we've build them in the past.    

In the past we developers had a lot of safety guarantees that made our job a lot easier. Everything ran mostly co-located. When things failed, we knew were to look and could reason about stack traces from our co-located components. We didn't have to reason much about concurrency or failure conditions with our data because the database solved a lot of that for us. When we called application or infrastructure services we did so with confidence because we were told those services here highly available (whatever that really meant). When we made changes to the data owned by our application, those changes were instantly visible to other components in our application. We could make changes to many components with a single deployment since we owned all of the application. And there are many other assumptions we've lived with, many of which will not hold up in the architectures designed for the cloud era.  

When we think about designing applications in a cloud-native, world we focus on three main things:

1. Speed of change - can we make changes to the overall system without impacting other applications or services
2. Elasticity - we should be able to take advantage of additional infrastructure and scale our applications horizontally
3. Failure - things can and do fail all the time whether it's computers, network, storage, or just as likely: our own applications

For this article, I'll focus mostly on the last bullet: designing for failure. 

When we build cloud-native applications we are first and foremost building a distributed system. Communication between services and infrastructure happens over the network. Failure happens in all shapes and sizes. Hardware fails. Applications have bugs. Things can be working perfectly fine but to an observer across the network they appear to be failed. But services need to communicate and cooperate with each other to make progress. In many cases they share similar concepts and data. For example, for an online bookstore, a book may be represented in different parts of the system differently. We may have a search service, a recommendation service, and a checkout/order service. All of these have a different understanding about what is a book and how to interpret data related to books. A problem arises when certain details of a book change. All of these services that store information related to a particular book may need to be updated with the new information. How do we deal with this? 

Another issue that arises in these distributed, cloud-native architectures is when we have services that rely on the availability of their downstream dependencies. For example, if service A calls service B and C, and cannot make any material progress without getting responses from both B and C then what do do in the face of failures? Return an error saying we cannot proceed because other parts of the system are down? 

As developers in this new cloud era, we have to revisit some of the motivations for why we had it much easier in the past. We'll find that this new cloud-native model blows up may of these previously held assumptions. We must return to the drawing board and be much more steeped in distributed systems practices. For example, although we've probably heard about the CAP theorem, what does it have to do with cloud-native applications? Above, we were discussing interaction patterns between services especially where distributed data is involved. CAP tells us we can "pick 2 out of" consistency, availability, and partition tolerance for data consistency. This may be a good "jumping-off" point, but it's not sufficient. Consistency comes in many forms: Linearizability, sequential, causal, PRAM, read-your writes, eventual, and many others. The CAP theorem discusses trade-offs in terms of linearizability alone. What about the other consistency models? Are they appropriate? Do they help with some of the issues described above? Doug Terry has a great paper about replicated consistency models in the real life that shed some light on this. 

If you build the ability to tolerate failure into the system as a first-class citizen, then you have to accept that data across these services may not always be strictly consistent. We should still be able to move forward, make progress, and provide some level of service to our service consumers and ultimately to our customers. If I make a call to the recommendation service should it just fail because some of its dependencies are not available? Or does it still have the responsibility to provide some level of "service"? In many ways, services make promises about what they intend to do. There are no absolutes. In the face of failures our services try to do what they can with what they have. Mark Burgess discusses this deeper in his explanation of "promise theory". Services make promises to each other and in the event they cannot keep their promises, each individual service must do what it can independently to keep its promise. Promise theory makes very explicit the nature of uncertainty and failure in these types of systems. We need to build services from the onset with promises in mind. 
 
If we have failures as a first-class design consideration and we have relaxed consistency models to be able to contend with it, we absolutely can end up in situations where we make a decision that have to be reverted. For example, if I order a widget on a popular online retailer and they take my order, what happens if they cannot fulfill it? What happens if they run out of stock? Or if it gets lost in delivery? Or they never had any in stock in the first place? In our previous life of comfort and safety we could pessimistically enforce that these situations do not occur. In a complex distributed system, these scenarios can and will crop up. But are they so bad? In the business world, we have ways to work around this. We may just resend the widget if it gets lost in delivery. We may go to our competitor and buy the widget from them and send it along to you in an effort to keep our promise. Have you ever heard of "overbooked" airplanes? If everyone shows up they offer money for people to take a bump. In the real world we deal with these sort of inconsistencies as they pop up and as part of doing business. Why shouldn't we do so in a distributed system environment?

The point of this article was to get you thinking about how distributed environments work in the real world and how we have the tools and practices to acomplish the same in distributed computing. This is not for the faint of heart. The long-standing safety guarantees of our past world should not be given up lightly if not needed. But I suspect in this new cloud world the focus on speed of change, resiliency, and scalability will preclude us from ignoring distributed systems. 